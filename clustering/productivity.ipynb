{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Aim of this notebook is to gather metrics related to productivity.\n",
    "#In particular, will try to structre around a 'cycle' of low/high productivity.\n",
    "#Hypothesised cycle: productivity -> pay -> living standards -> attainment/skills -> productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LUDA data prod metrics that may be worth including\n",
    "#Productivity: GVA, high growth business, innovation\n",
    "#Pay: Weekly pay, domestic income, NEET\n",
    "#Living standards: Healthy LE, physical activity, green space, employment rate, life satisfaction\n",
    "#Attaiment & skills: GSCEs, 5yr old standards, apprenticeships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get data -- first from published subnat explorer.\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "client = bigquery.Client(location=\" europe-west2\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT AREACD, Indicator, Value \n",
    "    FROM `project.ingest_dataset_name.ingest_table_name` \n",
    "    \n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"location\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "subnat_explorer_metrics = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helpful functions for moving metrics about.\n",
    "def cut_single_metric(name):\n",
    "    return subnat_explorer_metrics[subnat_explorer_metrics['Indicator']==name]\n",
    "\n",
    "def query_to_dataframe(query):\n",
    "    client = bigquery.Client(location=\"location\")\n",
    "    query_job = client.query(query, location=\"location\",)  \n",
    "    return(query_job.to_dataframe())\n",
    "\n",
    "def get_most_recent(data, year_syntax='YEAR'):\n",
    "    most_recent_year = data[data[year_syntax] == data[year_syntax].max()]\n",
    "    return(most_recent_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Productivity\n",
    "gva = cut_single_metric(\"Gross Value Added per hour worked\")\n",
    "query = \"\"\"SELECT GEOGRAPHY, YEAR, HIGH_GROWTH_BUSINESSES as Value FROM `project.ingest_dataset_name.growth_table_name`\"\"\"\n",
    "high_growth = query_to_dataframe(query)\n",
    "high_growth = get_most_recent(high_growth)\n",
    "high_growth['Indicator']='High growth businesses'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pay\n",
    "weekly_pay = cut_single_metric(\"Gross median weekly pay\")\n",
    "\n",
    "query=\"\"\"SELECT LAD_CODE, YEAR, GDI_PER_HEAD as Value FROM `project.ingest_dataset_name.gdi_table_name`\"\"\"\n",
    "gdi = get_most_recent(query_to_dataframe(query))\n",
    "gdi['Indicator'] = 'GDI per head'\n",
    "\n",
    "query=\"\"\"SELECT ONS_GEOGRAPHY_CODE_9_DIGIT, YEAR, PROPORTION_NEET_OR_NOT_KNOWN as Value FROM `project.ingest_dataset_name.46_neet`\"\"\"\n",
    "neet = get_most_recent(query_to_dataframe(query))\n",
    "#For some reason, looks like this table has duplicated rows.\n",
    "neet = neet.drop_duplicates()\n",
    "neet['Indicator'] = 'Proportion NEET'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Living standards\n",
    "\n",
    "male_hle = cut_single_metric(\"Male healthy life expectancy\")\n",
    "female_hle = cut_single_metric(\"Female healthy life expectancy\")\n",
    "satisfaction = cut_single_metric(\"Average life satisfaction rating\")\n",
    "\n",
    "#Think we've questioned this metric for rural areas...\n",
    "query=\"\"\"SELECT LAD_CODE, AVERAGE_DISTANCE_TO_NEAREST_PARK_OR_PUBLIC_GARDEN_OR_PLAYING_FIELD_M as Value FROM `ons-luda-data-prod.ingest_luda.49-green_space`\"\"\"\n",
    "green_space = query_to_dataframe(query)\n",
    "green_space['Indicator'] = 'Avergae distance to park or public garden or playing field'\n",
    "\n",
    "query=\"\"\"SELECT CODE, YEAR, ACTIVITY, RATE_PERCENTAGE as Value FROM `project.ingest_dataset_name.44_physical_activity`\"\"\"\n",
    "activity = get_most_recent(query_to_dataframe(query))\n",
    "#Take inactive metric as proportion inactive\n",
    "inactive = activity[activity[\"ACTIVITY\"]==\"Inactive\"]\n",
    "inactive.pop(\"ACTIVITY\")\n",
    "inactive['Indicator'] = 'Proportion inactive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Attainment/skills\n",
    "gcses = cut_single_metric(\"Young people achieving GCSEs (and equivalent qualifications) in English and Maths by age 19\")\n",
    "\n",
    "maths_5 = cut_single_metric(\"5 year olds achieving 'expected level' on maths early learning goals\")\n",
    "lit_5 = cut_single_metric(\"5 year olds achieving 'expected level' on literacy early learning goals\")\n",
    "comm_5 = cut_single_metric(\"5 year olds achieving 'expected level' on communication early learning goals\")\n",
    "\n",
    "employ_rate = cut_single_metric(\"Employment rate for 16 to 64 year olds\")\n",
    "\n",
    "app_start = cut_single_metric(\"Number of completions on apprenticeships\") #Note: need a rate?\n",
    "app_completion = cut_single_metric(\"Number of starts on apprenticeships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#What we want to do next in check coverage and coherence.\n",
    "#Think HLE is at Upper-tier local authority in England.\n",
    "\n",
    "#def get_number_area(dataset, )\n",
    "def get_code_column(dataset, flag='E0'):\n",
    "    \"\"\"Usage: The flag is a substring that we want to identify geography codes. \n",
    "    Default is E0, which is the prefix of areas in England, \n",
    "    but can be specified otherwise if data isn't at that level.\"\"\"\n",
    "    #Note use of groupby here as there was a deprecation warning for all(level=1), suggesting groupby(level=1).all() is safer.\n",
    "    #But beware that default behaviour of groupby is to sort alphabetically, which we very much don't want!\n",
    "    col = dataset.columns[dataset.stack().str.contains(flag).groupby(level=1, sort=False).any()]\n",
    "    if len(col)==0:\n",
    "        print(\"ERROR: No columns that look like the contain geography codes found. Checking if the flag entered matches the expected pattern\")\n",
    "    return(col)\n",
    "\n",
    "def number_areas(dataset, flag='E0'):\n",
    "    area_col = get_code_column(dataset, flag)\n",
    "    if len(area_col)==0:\n",
    "        return(0)\n",
    "    areas = dataset[area_col].drop_duplicates()\n",
    "    return(len(areas))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"gva\", \"high_growth\", \"weekly_pay\", \"gdi\", \"neet\", \"employ_rate\", \n",
    "           \"male_hle\", \"female_hle\", \"gcses\", \"maths_5\", \"lit_5\", \"comm_5\", \"app_start\", \"app_completion\",\n",
    "          \"satisfaction\", \"green_space\", \"inactive\"]\n",
    "for metric in metrics:\n",
    "    print(metric, number_areas(eval(metric)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tells us where we have UTLA (upper tier LA) data, as well as hinting on coverage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get consistent coverage.\n",
    "#What seems like it should work be England, lower tier LA.\n",
    "upp_low_tier = pd.read_csv(\"lookup/Lower_Tier_Local_Authority_to_Upper_Tier_Local_Authority__April_2019__Lookup_in_England_and_Wales.csv\")\n",
    "upp_low_tier = upp_low_tier[[\"LTLA19CD\", \"UTLA19CD\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Simplest way to model this would be when we have UT data, model LT data as all equal to UT.\n",
    "#This seems easy to justify as best guess.\n",
    "#For similarity work, this effectively weights UT only data by number of LT areas it corresponds to.\n",
    "#It also produces identical lower tier LAs as an artefact?\n",
    "all_UT = upp_low_tier[\"UTLA19CD\"].drop_duplicates()\n",
    "\n",
    "def UT_metric_to_LT(metric):\n",
    "    lower_metrics=[]\n",
    "    area_col = get_code_column(metric)\n",
    "    print(area_col)\n",
    "    for row in range(len(metric)):\n",
    "        #Test to see if this a upper tier LA. If it is, replace it with lower tier.\n",
    "        if metric.iloc[row].loc[area_col][0] in list(all_UT):\n",
    "            lower_metric = metric.reset_index().truncate(row,row).merge(upp_low_tier, left_on=area_col[0], right_on=\"UTLA19CD\", how=\"left\")\n",
    "            lower_metric = lower_metric.drop(columns=[area_col[0], 'UTLA19CD'])\n",
    "            col = lower_metric.pop('LTLA19CD')\n",
    "            lower_metric.insert(1, area_col[0], col)\n",
    "            lower_metrics.append(lower_metric)\n",
    "        else:\n",
    "            lower_metric = metric.reset_index().truncate(row,row).merge(upp_low_tier, left_on=area_col[0], right_on=\"UTLA19CD\", how=\"left\")\n",
    "            lower_metric = lower_metric.drop(columns=['LTLA19CD', 'UTLA19CD'])\n",
    "            lower_metrics.append(lower_metric)\n",
    "    return(pd.concat(lower_metrics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK, convert to LT local authority\n",
    "male_hle = UT_metric_to_LT(male_hle)\n",
    "female_hle = UT_metric_to_LT(female_hle)\n",
    "neet = UT_metric_to_LT(neet)\n",
    "lit_5 = UT_metric_to_LT(lit_5)\n",
    "maths_5 = UT_metric_to_LT(maths_5)\n",
    "comm_5 = UT_metric_to_LT(comm_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat this check of the number of areas covered by each metric.\n",
    "for metric in metrics:\n",
    "    print(metric, number_areas(eval(metric)))\n",
    "#Now believe that the differences come from coverage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To combine everything up, need consistent format. Metrics from subnat explorer have one, but there are others.\n",
    "#Those have columns of AREACD, Indicator, Value, so this should return True and 3 for these.\n",
    "for metric in metrics:\n",
    "    print( metric, get_code_column(eval(metric))==\"AREACD\", len(list(eval(metric))) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_index_column(metric, col_to_drop='index'):\n",
    "    if col_to_drop in list(metric):\n",
    "        return(metric.drop(columns=col_to_drop))\n",
    "    else:\n",
    "        return(metric)\n",
    "        \n",
    "lit_5 = drop_index_column(lit_5)\n",
    "maths_5 = drop_index_column(maths_5)\n",
    "comm_5 = drop_index_column(comm_5)\n",
    "neet = drop_index_column(neet)\n",
    "male_hle = drop_index_column(male_hle)\n",
    "female_hle = drop_index_column(female_hle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neet = drop_index_column(neet, 'YEAR')\n",
    "gdi = drop_index_column(gdi, 'YEAR')\n",
    "inactive = drop_index_column(inactive, 'YEAR')\n",
    "high_growth = drop_index_column(high_growth, 'YEAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This takes the area column, renames in to 'AREACD' as in consolidated dataset. \n",
    "#And sticks it in the front of the dataframe, in case it isn't already\n",
    "def harmonise_area_col_name(metric):\n",
    "    area_col=get_code_column(metric)[0]\n",
    "    col = metric.pop(area_col)\n",
    "    metric.insert(0, \"AREACD\", col)\n",
    "    return(metric)\n",
    "\n",
    "high_growth = harmonise_area_col_name(high_growth)\n",
    "gdi = harmonise_area_col_name(gdi)\n",
    "neet = harmonise_area_col_name(neet)\n",
    "green = harmonise_area_col_name(green_space)\n",
    "inactive = harmonise_area_col_name(inactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check this again.\n",
    "#Those have columns of AREACD, Indicator, Value, so this should return True and 3 for these.\n",
    "for metric in metrics:\n",
    "    print( metric, get_code_column(eval(metric))==\"AREACD\", len(list(eval(metric))) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert apprenticeships into a rate\n",
    "app_rate = app_completion.merge(app_start, on=\"AREACD\", how='inner')\n",
    "app_rate['Value'] = app_rate['Value_x']/app_rate['Value_y']\n",
    "app_rate['Indicator'] = 'Approx apprenticeship completion rate'\n",
    "app_rate.pop('Value_x')\n",
    "app_rate = app_rate[['AREACD', 'Indicator', 'Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ONly problem left is with NEET, which isn't numeric data. It is string (includes NaN for Wales).\n",
    "neet['Value'] = pd.to_numeric(neet['Value'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Concatanate cleaned(?!) data into groups.\n",
    "productivity = [gva, high_growth]\n",
    "pay = [weekly_pay, gdi, employ_rate, neet]\n",
    "lstandards =[male_hle, female_hle, satisfaction, green_space, inactive]\n",
    "skills = [gcses, app_rate, maths_5, lit_5, comm_5]\n",
    "\n",
    "productivity = pd.concat(productivity)\n",
    "pay = pd.concat(pay)\n",
    "lstandards = pd.concat(lstandards)\n",
    "skills = pd.concat(skills)\n",
    "\n",
    "all_metrics = pd.concat([productivity, pay, lstandards, skills])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Want these to be formatted as pivot tables\n",
    "productivity = pd.pivot_table(productivity, values='Value', columns='Indicator', index='AREACD').reindex()\n",
    "pay = pd.pivot_table(pay, values='Value', columns='Indicator', index='AREACD').reindex()\n",
    "lstandards = pd.pivot_table(lstandards, values='Value', columns='Indicator', index='AREACD').reindex()\n",
    "skills = pd.pivot_table(skills, values='Value', columns='Indicator', index='AREACD').reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get geospatial data for local authorities via a query.\n",
    "import geopandas\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT LAD20CD, geom, BNG_E, BNG_N\n",
    "    FROM `project.ingest_geography_dataset.geography_table`\n",
    "    \n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"location\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "la_geo = query_job.to_geodataframe\n",
    "la_geo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Basic clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, n_init=10, max_iter=300)\n",
    "\n",
    "#As we're using non-determintic process, set the seed for reproducibility\n",
    "import numpy as np\n",
    "np.random.seed(seed=19042022)\n",
    "\n",
    "def get_clusters(metrics):\n",
    "    no_na_metrics = metrics[metrics.notna().all(axis=1)]\n",
    "    scaler = StandardScaler()\n",
    "    metrics_scaled = scaler.fit_transform(no_na_metrics)\n",
    "    kmeans.fit(metrics_scaled)\n",
    "    clusters = pd.DataFrame(no_na_metrics.reset_index()['AREACD'])\n",
    "    clusters['Cluster'] = kmeans.labels_\n",
    "    clusters = la_geo().merge(clusters, right_on = 'AREACD', left_on = 'LAD20CD', how='right')\n",
    "    return(clusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK, use same cluster numbers and see what you get.\n",
    "#Note: the default colour map used by geopandas appears to be viridis.\n",
    "\n",
    "prod_cluster = get_clusters(productivity)\n",
    "prod_cluster.plot(column = \"Cluster\", cmap=\"Spectral\")\n",
    "\n",
    "pay_cluster = get_clusters(pay)\n",
    "pay_cluster.plot(column = \"Cluster\", cmap=\"Spectral\")\n",
    "\n",
    "lstandards_cluster = get_clusters(lstandards)\n",
    "lstandards_cluster.plot(column = \"Cluster\", cmap=\"Spectral\")\n",
    "\n",
    "skills_cluster = get_clusters(skills)\n",
    "skills_cluster.plot(column = \"Cluster\", cmap=\"Spectral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is simple as there are only two metrics classed under productivity, so the scatter should be clean.\n",
    "prod_analysis = prod_cluster.merge(productivity, on=\"AREACD\")\n",
    "prod_analysis.plot.scatter(x=\"Gross Value Added per hour worked\", \n",
    "                           y=\"High growth businesses\",\n",
    "                           c=\"Cluster\", cmap=\"Spectral\")\n",
    "prod_cluster.plot(column = \"Cluster\", cmap=\"Spectral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This isn't the best way to do this, as cluster labelling non-determistic and meaningless.\n",
    "#But for now use a dict to 'rank' the clusters in order of how well they perform.\n",
    "#Simple here, split by GVA and then high growth businesses\n",
    "prod_ranking={3:1, 2:2, 0:3, 1:4, 4:5}\n",
    "prod_cluster['Ranked clusters']=prod_cluster['Cluster'].map(prod_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More complexity here as we have 5 metrics.\n",
    "skill_analysis = skills_cluster.merge(skills, on=\"AREACD\")\n",
    "skill_analysis\n",
    "#This shows little\n",
    "skill_analysis.plot.scatter(y=\"Approx apprenticeship completion rate\", \n",
    "                           x=\"Young people achieving GCSEs (and equivalent qualifications) in English and Maths by age 19\",\n",
    "                           c=\"Cluster\", cmap=\"Spectral\")\n",
    "#This shows clearer a picture of what the split looks like.\n",
    "skill_analysis.plot.scatter(y=\"Approx apprenticeship completion rate\", \n",
    "                           x=\"5 year olds achieving 'expected level' on communication early learning goals\",\n",
    "                           c=\"Cluster\", cmap=\"Spectral\")\n",
    "skills_cluster.plot(column = \"Cluster\", cmap=\"Spectral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skills_ranking={3:1, 1:2, 4:3, 2:4, 0:5}\n",
    "#Middle three clusters similar here. 2nd worst marked by low GCSEs, 2nd best by high GCSEs. \n",
    "#Middle one (3rd) has higher apprenticeships.\n",
    "skills_cluster['Ranked clusters']=skills_cluster['Cluster'].map(skills_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstandard_analysis = lstandards_cluster.merge(lstandards, on=\"AREACD\")\n",
    "lstandard_analysis.plot.scatter(y=\"Average life satisfaction rating\", \n",
    "                           x=\"Female healthy life expectancy\",\n",
    "                           c=\"Cluster\", cmap=\"Spectral\")\n",
    "lstandard_analysis.plot.scatter(y=\"Proportion inactive\", \n",
    "                           x=\"Female healthy life expectancy\",\n",
    "                           c=\"Cluster\", cmap=\"Spectral\")\n",
    "lstandards_cluster.plot(column = \"Cluster\", cmap=\"Spectral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls_ranking={4:1, 2:2, 0:3, 3:4, 1:5}\n",
    "#2nd worst cluster seperated by satisfaction from 3rd/4th. 3rd/4th split by inactivity...\n",
    "lstandards_cluster['Ranked clusters']=lstandards_cluster['Cluster'].map(ls_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_analysis = pay_cluster.merge(pay, on=\"AREACD\")\n",
    "pay_analysis.plot.scatter(y=\"Proportion NEET\", \n",
    "                           x=\"Gross median weekly pay\",\n",
    "                           c=\"Cluster\", cmap=\"Spectral\")\n",
    "pay_analysis.plot.scatter(y=\"Employment rate for 16 to 64 year olds\", \n",
    "                           x=\"Gross median weekly pay\",\n",
    "                           c=\"Cluster\", cmap=\"Spectral\")\n",
    "pay_cluster.plot(column = \"Cluster\", cmap=\"Spectral\")\n",
    "#Employment rate for 16 to 64 year olds\n",
    "#Proportion NEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pay_ranking={0:1, 1:2, 3:3, 4:4, 2:5}\n",
    "#3rd cluster spread across pay and employment, marked by high NEET.\n",
    "#Lowest cluster similar in pay to 2nd; split by lower employment.\n",
    "pay_cluster['Ranked clusters']=pay_cluster['Cluster'].map(pay_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Combine the clusters to look for correspondance?\n",
    "#Use ranked clusters here, which I have generated by eye for now.\n",
    "cluster_col='Ranked clusters'\n",
    "combine_clusters = prod_cluster[['AREACD',cluster_col]].merge(pay_cluster[['AREACD',cluster_col]], on='AREACD')\n",
    "combine_clusters = combine_clusters.merge(lstandards_cluster[['AREACD',cluster_col]], on='AREACD')\n",
    "combine_clusters = combine_clusters.merge(skills_cluster[['AREACD',cluster_col]], on='AREACD')\n",
    "combine_clusters.columns = [['AREACD','prod', 'pay', 'lstandards', 'skills']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix errors by reseting indices\n",
    "combine_clusters.columns = combine_clusters.columns.get_level_values(0)\n",
    "\n",
    "#Function to generate bubble plots -- scatters with the point size given by number obs.\n",
    "def plot_bubbles(x='prod', y='pay', clusters=combine_clusters, normalise=False):\n",
    "    weights=pd.DataFrame(clusters[[x,y]].value_counts())\n",
    "    weights=weights.reset_index()\n",
    "    weights.columns=[x,y,'count']\n",
    "    if normalise:\n",
    "        weights2=pd.DataFrame(clusters[[x]].value_counts())\n",
    "        weights2=weights2.reset_index()\n",
    "        weights2.columns=[x,'total']\n",
    "        weights = weights.merge(weights2, on=x)\n",
    "        weights['weight']=weights['count']/weights['total']\n",
    "    else:\n",
    "        weights['weight']=weights['count']\n",
    "    weights['weight']/=max(weights['weight'])*0.005\n",
    "    weights.plot.scatter(x, y, s='weight', c='count', cmap='viridis')\n",
    "\n",
    "#Probably most enlightening to put the productivity on the x axis and compare to all others?\n",
    "plot_bubbles('prod', 'pay')\n",
    "plot_bubbles('prod', 'skills')\n",
    "plot_bubbles('prod', 'lstandards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the living standards on the x axis.\n",
    "plot_bubbles('lstandards', 'prod')\n",
    "plot_bubbles('lstandards', 'skills')\n",
    "plot_bubbles('lstandards', 'pay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the skills on the x axis.\n",
    "plot_bubbles('skills', 'prod')\n",
    "plot_bubbles('skills', 'lstandards')\n",
    "plot_bubbles('skills', 'pay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the pay on the x axis.\n",
    "plot_bubbles('pay', 'prod')\n",
    "plot_bubbles('pay', 'skills')\n",
    "plot_bubbles('pay', 'lstandards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each area, make a simple score by summing the cluster ranks for a total between 4 and 20.\n",
    "combine_clusters['score'] = combine_clusters['pay']+combine_clusters['prod']+combine_clusters['skills']+combine_clusters['lstandards']\n",
    "combine_clusters['score'].plot.hist(bins=combine_clusters['score'].nunique())\n",
    "#combine_clusters.hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peak at 14, but this will include a number of different ways to add up to 14.\n",
    "#14 with score of 20, another 21 with 19.\n",
    "#Can see these all have 5 for living standards.\n",
    "combine_clusters[combine_clusters['score']==19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For a reference, here is the documnetation related to silhouettes used below.\n",
    "#https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OK, let's do some further work on clustering.\n",
    "#Open issues -- we chose 5 clusters by hand. We also `labelled` them by visual inspection. There are gaps in the maps (believe boundary changes are the cause)\n",
    "#Start with the easiest issue -- optimising N_clusters.\n",
    "#Start with the prod as it's the simplest.\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, n_init=10, max_iter=300)\n",
    "np.random.seed(seed=19042022)\n",
    "\n",
    "def get_clusters(metrics):\n",
    "    no_na_metrics = metrics[metrics.notna().all(axis=1)]\n",
    "    scaler = StandardScaler()\n",
    "    metrics_scaled = scaler.fit_transform(no_na_metrics)\n",
    "    kmeans.fit(metrics_scaled)\n",
    "    clusters = pd.DataFrame(no_na_metrics.reset_index()['AREACD'])\n",
    "    clusters['Cluster'] = kmeans.labels_\n",
    "#    clusters = la_geo().merge(clusters, right_on = 'AREACD', left_on = 'LAD20CD', how='right')\n",
    "    return(clusters)\n",
    "\n",
    "prod_test = get_clusters(productivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_no_na = productivity[productivity.notna().all(axis=1)]\n",
    "labels = kmeans.fit_predict(prod_no_na)\n",
    "silhouette_score(prod_no_na, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clusters in range(2,10):\n",
    "    kmeans = KMeans(n_clusters, n_init=10, max_iter=300)\n",
    "    scaler = StandardScaler()\n",
    "    prod_scaled = scaler.fit_transform(prod_no_na)\n",
    "    kmeans.fit(prod_scaled)\n",
    "    labels = kmeans.fit_predict(prod_no_na)\n",
    "    print(n_clusters, silhouette_score(prod_no_na, labels))\n",
    "    labels = get_clusters(productivity)\n",
    "    analysis = labels.merge(productivity, on=\"AREACD\")\n",
    "    analysis.plot.scatter(x=\"Gross Value Added per hour worked\", \n",
    "                           y=\"High growth businesses\",\n",
    "                           c=\"Cluster\", cmap=\"Spectral\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_no_na = skills[skills.notna().all(axis=1)]\n",
    "\n",
    "\n",
    "for n_clusters in range(2,10):\n",
    "    kmeans = KMeans(n_clusters, n_init=10, max_iter=300)\n",
    "    scaler = StandardScaler()\n",
    "    skill_scaled = scaler.fit_transform(skill_no_na)\n",
    "    kmeans.fit(skill_scaled)\n",
    "    labels = kmeans.fit_predict(skill_no_na)\n",
    "    print(n_clusters, silhouette_score(skill_no_na, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_no_na = lstandards[lstandards.notna().all(axis=1)]\n",
    "\n",
    "for n_clusters in range(2,10):\n",
    "    kmeans = KMeans(n_clusters, n_init=10, max_iter=300)\n",
    "    scaler = StandardScaler()\n",
    "    ls_scaled = scaler.fit_transform(ls_no_na)\n",
    "    kmeans.fit(ls_scaled)\n",
    "    labels = kmeans.fit_predict(ls_no_na)\n",
    "    print(n_clusters, silhouette_score(ls_no_na, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_no_na = pay[pay.notna().all(axis=1)]\n",
    "\n",
    "for n_clusters in range(2,10):\n",
    "    kmeans = KMeans(n_clusters, n_init=10, max_iter=300)\n",
    "    scaler = StandardScaler()\n",
    "    pay_scaled = scaler.fit_transform(pay_no_na)\n",
    "    kmeans.fit(pay_scaled)\n",
    "    labels = kmeans.fit_predict(pay_no_na)\n",
    "    print(n_clusters, silhouette_score(pay_no_na, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X = ls_scaled\n",
    "distorsions = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(X)\n",
    "    distorsions.append(kmeans.inertia_)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.plot(range(2, 10), distorsions)\n",
    "plt.grid(True)\n",
    "plt.title('Elbow curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Experiment with radar plots.\n",
    "#Get some clusters\n",
    "n_clusters=5\n",
    "ls_no_na = lstandards[lstandards.notna().all(axis=1)]\n",
    "kmeans = KMeans(n_clusters, n_init=10, max_iter=300)\n",
    "scaler = StandardScaler()\n",
    "ls_scaled = scaler.fit_transform(ls_no_na)\n",
    "kmeans.fit(ls_scaled)\n",
    "labels = kmeans.fit_predict(ls_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centres = kmeans.cluster_centers_\n",
    "\n",
    "categories = lstandards.columns\n",
    "categories = [*categories, categories[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster1 = centres[0].tolist()\n",
    "cluster1 = [*cluster1, cluster1[0]]\n",
    "\n",
    "cluster2 = centres[1].tolist()\n",
    "cluster2 = [*cluster2, cluster2[0]]\n",
    "\n",
    "all_clusters=[]\n",
    "for i in range(len(centres)):\n",
    "    cluster_i = centres[i].tolist()\n",
    "    cluster_i = [*cluster_i, cluster_i[0]]\n",
    "    all_clusters.append(cluster_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_loc = np.linspace(start=0, stop=2 * np.pi, num=len(cluster1))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(polar=True)\n",
    "for i in range(len(centres)):\n",
    "    plt.plot(label_loc, all_clusters[i], label='Cluster'+str(i))\n",
    "plt.title('Test clusters radar plot', size=20, y=1.05)\n",
    "lines, labels = plt.thetagrids(np.degrees(label_loc), labels=categories)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
